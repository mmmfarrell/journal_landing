% !TEX root=./root.tex

As mentioned in~\secref{sec:BLAH}, the propsed estimator estimates both the state of
the UAV and the state of the landing vehicle as well as the positions of visual
landmarks on the landing vehicle.
% in the same Error-State Kalman
% filter~\cite{sola2017quaternion}.
We express the state of the combined estimated system as the tuple
\begin{equation}
  \hat{\x} =
  \begin{pmatrix}
    \hat{\x}_{\text{UAV}}, \hat{\x}_{\text{Goal}}, \hat{\x}_{\text{Landmarks}}
  \end{pmatrix}
\end{equation}
with the components defined as
\begin{align}
  \hat{\vect{x}}_{\text{UAV}} &=
  \begin{pmatrix}
    \hat{\vect{p}}_{b/I}^{I}, \hat{\vect{q}}_I^{b}, \hat{\vect{v}}_{b/I}^b,
    \hat{\vect{\beta}}_a,
    \hat{\vect{\beta}}_{\omega}
  \end{pmatrix}
  \in \mathbb{R}^3 \times S^3 \times \mathbb{R}^3 \times \mathbb{R}^3 \times
    \mathbb{R}^3  \\
  % \x_{\text{UAV}} &=
  % \begin{bmatrix}
    % \vect{p}_{b/I}^I &
    % \phi & \theta & \psi &
    % \vect{v}_{b/I}^b &
    % \mu & \vect{\beta}_a & \vect{\beta}_\omega
  % \end{bmatrix}^\transpose \\
    \hat{\x}_{\text{Goal}} & =
    \begin{pmatrix}
      \hat{\vect{p}}_{g/b}^{v}, \hat{\vect{v}}_{g/I}^{g}, \theta_{I}^{g},
      \omega_{g/I}^{g}
    \end{pmatrix}
    \in \mathbb{R}^3 \times \mathbb{R}^2 \times S^1 \times \mathbb{R}^1
    \\
    \hat{\x}_{\text{Landmarks}} & =
    \begin{pmatrix}
      \hat{\vect{r}}_{1/g}^{g}, \dots \hat{\vect{r}}_{n/g}^{g}
    \end{pmatrix}
    \in \mathbb{R}^3 \dots \mathbb{R}^3.
\end{align}
Note that $\hat{\vect{x}}_{\text{UAV}}$ contains the same states mentioned previously
in~\secref{sec:UAV_dynamics} with the addition of $\hat{\vect{\beta}}_a$ and
$\hat{\vect{\beta}}_\omega$, the estimated bias vectors for the acclerometer and
gyroscope sensors. On the
other hand, $\hat{\vect{x}}_{\text{Goal}}$ varies 
from the landing vehicle states mentioned in~\secref{sec:landing_veh_dynamics}
by containing $\hat{\vect{p}}_{g/b}^v$ instead of $\hat{\vect{p}}_{b/I}^I$.
% as well as $\hat{\vect{r}}_{1/g}^{g} \dots \hat{\vect{r}}_{n/g}^{g}$.
We estimate the relative state, $\hat{\vect{p}}_{g/b}^v$, instead
of the global state, $\hat{\vect{p}}_{g/I}^I$, as the relative state is observable
even with poor estimates of the UAV's global position, $\hat{\vect{p}}_{b/I}^I$.
The estimated vectors $\hat{\vect{r}}_{1/g}^{g} \dots \hat{\vect{r}}_{n/g}^{g}$ represent the
locations of visual landmarks $1 \dots n$ which are rigidly attached to the
landing vehicle. We show in our simulation and hardware experiments, that the
addition of these visual landmarks to the estimated state allows the estimator
to maintain accurate and consistent estimates of $\hat{\x}_{\text{Goal}}$ even
while the fiducial landing marker is not detected for long periods of time. 
% As mentioned in~\secref{sec:intro??} the estimation of these
% visual landmarks allows the estimator to maintain accurate and consistent
% estimates of the landing vehicle even while the fiducial landing marker is not
% detected for long periods of time.

The inputs to the system are given by
\begin{equation*}
  \vect{u} = \begin{pmatrix} \bar{\vect{a}}_{b/I}^I, \bar{\vect{\omega}}_{b/I}^b \end{pmatrix} \in
        \mathbb{R}^3 \times \mathbb{R}^3,
\end{equation*}
which are directly measured from the inertial measurement unit (IMU) on the UAV.

As the estimated state is not a vector, but rather a tuple of Lie groups, we
employ the Error-State Kalman Filter (ESKF) as described in~\cite{BLAH}. We note
that the estimated state, however, is of dynamic size. As visual landmarks are
detected they are added to the state vector until a maximum size of the state
vector is reached. As the visual landmarks leave the field of view of the camera
or are otherwise no longer tracked, they are removed from the estimated state
vector, making room for new visual landmarks to be added. In the following
subsections, we describe the dynamic equations of the estimated state, the
initialization of certain states, and the
measurement models used to update the filter.
